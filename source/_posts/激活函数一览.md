---
title: 激活函数一览
tags:
  - 深度学习
categories:
  - 机器学习
mathjax: true
toc: true
date: 2018-01-14 11:47:59
---

非线性激活函数 Sigmoid、ReLU 及各种变体、ELU……

<!--more-->

## 1. 非线性激活函数的作用

- 如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。
- 如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

## 2. Sigmoid

- 公式：$f(z)=\frac{1}{1+e^{(-z)}},f'(z)=f(z)(1-f(z))$


- 特点：
  - 取值范围限定在 (0,1)
  - 在特征相差比较复杂或是相差不是特别大时效果比较好。
- 缺点：
  - 激活函数计算量大，反向传播求误差梯度时，求导涉及除法。
  - 反向传播时，很容易就会出现梯度消失（饱和神经元）的情况，从而无法完成深层网络的训练。
  - Sigmoid 输出不以零为中心的。

## 3. tanh

- 公式：$f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}=2*sigmoid(2x)-1$
- 特点：
  - 取值范围限定在 [-1,1]
  - tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。
  - 与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好。
- 缺点：
  - Tanh 函数也会有梯度消失的问题，因此在饱和时也会「杀死」梯度。

## 4. ReLU

- 公式：$f(z)=max(0,z)$
- 特点：
  - 使网络更快速地收敛。它不会饱和，即它可以对抗梯度消失问题，至少在正区域（x> 0 时）可以这样，因此神经元至少在一半区域中不会把所有零进行反向传播。由于使用了简单的阈值化（thresholding），ReLU 计算效率很高。
- 缺点：
  - ReLU 函数的输出不以零为中心（恒为正）。
  - 前向传导（forward pass）过程中，如果 x < 0，则神经元保持非激活状态，且在后向传导（backward pass）中「杀死」梯度。这样权重无法得到更新，网络无法学习。当 x = 0 时，该点的梯度未定义，但是这个问题在实现中得到了解决，通过采用左侧或右侧的梯度的方式。
  - Relu 不会对数据做幅度压缩，所以如果数据的幅度不断扩张，那么模型的层数越深，幅度的扩张也会越厉害，最终会影响模型的表现。

## 5. Leaky ReLU

- 公式：$f(z)=max(0.1x,x)$
- 特点：
  - 当 x < 0 时，它得到 0.1 的正梯度。该函数一定程度上缓解了 dead ReLU 问题。
  - 具备 ReLU 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。
- 缺点：
  - 该函数的结果并不连贯。

## 7. Parametrix ReLU

- 公式：$f(z)=max(\alpha x,x)$
- 特点：
  - 引入了一个随机的超参数 $\alpha$，它可以被学习，因为你可以对它进行反向传播。这使神经元能够选择负区域最好的梯度。

## 8. Swish

- 公式：$f(z)=\frac{x}{1+e^{-x}}$
- 特点：
  - Swish 激活函数的性能优于 ReLU 函数。

## 9. ELU

- 公式：$f(z)=x\geq0 \ ? \ x \ : \ \alpha(e^x-1),\alpha>0$
- 特点：
  - ELU 减少了正常梯度与单位自然梯度之间的差距，从而加快了学习。
  - 在负的限制条件下能够更有鲁棒性（0均值）。