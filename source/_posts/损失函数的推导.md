---
title: 损失函数的推导
tags:
  - 机器学习
categories:
  - 机器学习
mathjax: true
toc: true
date: 2018-01-17 11:50:27
---

log-loss、square-loss、exp-loss 及 hinge-loss 等。

<!--more-->

## 1. 损失函数的作用

损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用 $L(Y, f(x))$ 来表示，损失函数越小，模型的鲁棒性就越好。损失函数是**经验风险函数**的核心部分，也是**结构风险函数**重要组成部分。模型的结构风险函数包括了经验风险项和正则项：

$$\theta^* = \arg \min_\theta \frac{1}{N}{}\sum\limits_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\  \Phi(\theta)$$

其中，前面的均值函数表示的是经验风险函数，$L(y_i,f(x_i;\theta))$ 代表的是损失函数，后面的 $Φ(\theta)$ 是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是 L1，也可以是 L2，或者其他的正则函数。整个式子表示的意思是**找到使目标函数最小时的 $θ$ 值**

## 2. log 对数损失函数

- 逻辑回归（Logistic Regression, LR）
- 在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —-> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。
- `log-loss`：

$$L(Y,P(Y|X)) = -\log P(Y|X)$$

- 逻辑回归的概率表达式：

$$P(Y=1|x)=h_{\theta}(x)=g(f(x))=\frac{1}{1+e^{-f(x)}}, \ \log h_{\theta}(x)=-\log(1+e^{-f(x)}) $$

$$P(Y=0|x)=1-h_{\theta}(x)=1-g(f(x))=\frac{1}{1+e^{f(x)}}, \ \log (1-h_{\theta})=-\log(1+e^{f(x)}) $$

- 逻辑回归的的损失函数：

$$J(\theta)=-\frac{1}{m}\sum\limits^m_{i=1}[y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(x^{(i)}))]$$

## 3. 平方损失函数

- 线性回归的最小二乘法（Ordinary Least Squares）。
- 在线性回归中，它假设样本和噪声都服从高斯分布（中心极限定理），通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：**最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小**
- `square-loss`：

$$L(Y, f(X)) = (Y - f(X))^2$$

- 在实际应用中，通常会使用均方差（MSE）作为一项衡量指标：

$$MSE = \frac{1}{n} \sum\limits_{i=1} ^{n} (\tilde{Y_i} - Y_i )^2$$

## 4. 指数损失函数

- Adaboost
- `exp-loss`：

$$L(y,f(x))=e^{[-yf(x)]}$$

- 在 Adaboost 中，经过 $m$ 次迭代之后，可以得到 $f_m(x)$ ：

$$f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)$$

- Adaboost 每次迭代时的目的是为了找到最小化下列式子时的参数 $\alpha$ 和 $G$ ：

$$arg\min\limits_{\alpha,G}=\sum\limits^N_{i=1}e^{[-y_i(f_{m-1}(x_i))+\alpha G(x_i)]}$$

## 5. Hinge 损失函数

- 支持向量机（Support Vector Machine, SVM）
- `hinge-loss` ：

$$L(y) = \max(0, 1-y\tilde{y}), y=\pm 1$$

- 推导 SVM 的目标函数：

$$wx_++b\geq1- ξ, \ wx_-+b\le-1- ξ, \ 即 \  y_i(wx_i+b)\geq1- ξ$$

$$\min\limits_{w,b}\sum\limits^N_i\max(0,1-y_i(wx_i+b))+\lambda||w||^2$$

- 当 $ξ=1-y_i(wx_i+b)$ ，$\lambda=\frac{1}{2C}$ ,则目标函数就为：

$$\min\limits_{w,b}\frac{1}{C}(\frac{1}{2}||w||^2+C\sum\limits^N_{i=1}ξ_i)$$

## 6. 其他

- `0-1 loss` ：$L(Y,f(X))=(Y==f(X)) \ ? \ 1 \ : \ 0$
- `abs-loss` ：$L(Y,f(X))=|Y-f(X)|$

## 参考文章

- [机器学习-损失函数](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/) 注意文末的图